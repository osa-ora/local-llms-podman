# Deploy LLMs on your local machine 
---

We will explore different ways to download and chat with LLMs on your local machine.

1. Using Ollama: Check: ðŸ”— https://github.com/osa-ora/local-llms-podman/tree/main/ollama
     - Using Ollama Chat & APIs
     - Using Ollama Community Integrations
     - Using AnythingLLM with Ollama
     - Building a local Co-Pilot in VSC using Ollama
3. Using Podman Desktop Compose and Bee Agent Stack: check: ðŸ”— https://github.com/osa-ora/local-llms-podman/tree/main/bee-agent
4. Using Podman Desktop and AI Lab plugin: Check: ðŸ”— https://github.com/osa-ora/local-llms-podman/tree/main/podman-ai-lab

---
